{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "901cf5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c2f188",
   "metadata": {},
   "source": [
    "## Import Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d04d18c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../raw_data/dreaddit-train.csv\"\n",
    "test_filepath = \"../raw_data/dreaddit-test.csv\"\n",
    "\n",
    "df = pd.read_csv(filepath)\n",
    "df_test = pd.read_csv(test_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ecfcebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['text','label']\n",
    "\n",
    "df = df[columns]\n",
    "\n",
    "X_train = df[['text']]\n",
    "y_train = df[['label']]\n",
    "\n",
    "df_test = df_test[columns]\n",
    "X_test = df_test[['text']]\n",
    "y_test = df_test[['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8686864a",
   "metadata": {},
   "source": [
    "## Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2b0a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "X_train['lem_text'] = X_train['text'].apply(lambda x: lemmatize_text(x))\n",
    "X_test['lem_text'] = X_test['text'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d32b01e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lem_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He said he had not felt that way before, sugge...</td>\n",
       "      <td>he say he have not feel that way before , sugg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey there r/assistance, Not sure if this is th...</td>\n",
       "      <td>hey there r / assistance , not sure if this be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My mom then hit me with the newspaper and it s...</td>\n",
       "      <td>my mom then hit I with the newspaper and it sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>until i met my new boyfriend, he is amazing, h...</td>\n",
       "      <td>until I meet my new boyfriend , he be amazing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>October is Domestic Violence Awareness Month a...</td>\n",
       "      <td>October be Domestic Violence Awareness Month a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  He said he had not felt that way before, sugge...   \n",
       "1  Hey there r/assistance, Not sure if this is th...   \n",
       "2  My mom then hit me with the newspaper and it s...   \n",
       "3  until i met my new boyfriend, he is amazing, h...   \n",
       "4  October is Domestic Violence Awareness Month a...   \n",
       "\n",
       "                                            lem_text  \n",
       "0  he say he have not feel that way before , sugg...  \n",
       "1  hey there r / assistance , not sure if this be...  \n",
       "2  my mom then hit I with the newspaper and it sh...  \n",
       "3  until I meet my new boyfriend , he be amazing ...  \n",
       "4  October be Domestic Violence Awareness Month a...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac137fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'lem_text']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = list(X_train.columns)\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcca2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "stopword_list.remove('nor')\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]|\\[|\\]' if not remove_digits else r'[^a-zA-Z\\s]|\\[|\\]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    text = ' '.join(filtered_tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_text(text, \n",
    "#                    html_stripping=True, \n",
    "#                    contraction_expansion_2=True,\n",
    "#                    accented_char_removal=True, \n",
    "#                    text_lower_case=True,\n",
    "#                    text_stemming=False,\n",
    "                   text_lemmatization=True,\n",
    "#                    special_char_removal=True,\n",
    "#                    remove_digits=True,\n",
    "                   stopword_removal=True,\n",
    "                   stopwords=stopword_list):    # lemmatize text\n",
    "    \n",
    "    if text_lemmatization:\n",
    "        text = lemmatize_text(text)\n",
    "\n",
    "    # remove special characters and\\or digits\n",
    "#     if special_char_removal:\n",
    "#         # insert spaces between special characters to isolate them\n",
    "#         special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "#         text = special_char_pattern.sub(\" \\\\1 \", text)\n",
    "#         text = remove_special_characters(text, remove_digits=remove_digits)\n",
    "\n",
    "    # remove stopwords\n",
    "    if stopword_removal:\n",
    "        text = remove_stopwords(text, \n",
    "                                #is_lower_case=text_lower_case,\n",
    "                                stopwords=stopwords)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a08542ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import unicodedata\n",
    "from contractions import contractions_dict\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "import collections\n",
    "#from textblob import Word\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "stopword_list.remove('nor')\n",
    "\n",
    "\n",
    "\n",
    "#def correct_spellings_textblob(tokens):\n",
    "#\treturn [Word(token).correct() for token in tokens]\n",
    "\n",
    "def simple_porter_stemming(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "def remove_repeated_characters(text):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    text = ' '.join(correct_tokens)\n",
    "    return text\n",
    "\n",
    "def expand_contractions_2(text,contractions_dict=contractions_dict):\n",
    "    for word in text.split():\n",
    "        if word.lower() in contractions_dict:\n",
    "            text = text.replace(word, contractions_dict[word.lower()])\n",
    "    return text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]|\\[|\\]' if not remove_digits else r'[^a-zA-Z\\s]|\\[|\\]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    text = ' '.join(filtered_tokens)\n",
    "    return text\n",
    "\n",
    "def normalize_text(text, contraction_expansion_2=True,\n",
    "                     accented_char_removal=True, text_lower_case=True,\n",
    "                     text_stemming=False, text_lemmatization=True,\n",
    "                     special_char_removal=True, remove_digits=True,\n",
    "                     stopword_removal=True, stopwords=stopword_list):\n",
    "\n",
    "\n",
    "\n",
    "    # remove extra newlines\n",
    "    text = text.translate(text.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "\n",
    "    # remove accented characters\n",
    "    if accented_char_removal:\n",
    "        text = remove_accented_chars(text)\n",
    "\n",
    "    # expand contractions\n",
    "    if contraction_expansion_2:\n",
    "        text = expand_contractions_2(text)\n",
    "\n",
    "    # lemmatize text\n",
    "    if text_lemmatization:\n",
    "        text = lemmatize_text(text)\n",
    "\n",
    "    # stem text\n",
    "    if text_stemming and not text_lemmatization:\n",
    "        text = simple_porter_stemming(text)\n",
    "\n",
    "    # remove special characters and\\or digits\n",
    "    if special_char_removal:\n",
    "        # insert spaces between special characters to isolate them\n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        text = special_char_pattern.sub(\" \\\\1 \", text)\n",
    "        text = remove_special_characters(text, remove_digits=remove_digits)\n",
    "\n",
    "    # remove extra whitespace\n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "    # lowercase the text\n",
    "    if text_lower_case:\n",
    "        text = text.lower()\n",
    "\n",
    "    # remove stopwords\n",
    "    if stopword_removal:\n",
    "        text = remove_stopwords(text, is_lower_case=text_lower_case, stopwords=stopwords)\n",
    "\n",
    "    # remove extra whitespace\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d943be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['lem_text'] = X_train['text'].apply(lambda x: lemmatize_text(x))\n",
    "X_test['lem_text'] = X_test['text'].apply(lambda x: lemmatize_text(x))\n",
    "\n",
    "X_train['norm_text'] = X_train['text'].apply(lambda x: normalize_text(x))\n",
    "X_test['norm_text'] = X_test['text'].apply(lambda x: normalize_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a85f1",
   "metadata": {},
   "source": [
    "## Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c31c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_vector = vectorizer.fit_transform(X_train.text)\n",
    "X_test_vector = vectorizer.transform(X_test.text)\n",
    "\n",
    "X_train_vector_lemm = vectorizer.fit_transform(X_train.lem_text)\n",
    "X_test_vector_lemm = vectorizer.transform(X_test.lem_text)\n",
    "\n",
    "X_train_vector_norm = vectorizer.fit_transform(X_train.norm_text)\n",
    "X_test_vector_norm = vectorizer.transform(X_test.norm_text)\n",
    "\n",
    "data_sets = [(X_train_vector, X_test_vector), \n",
    "        (X_train_vector_lemm, X_test_vector_lemm),\n",
    "        (X_train_vector_norm, X_test_vector_norm)]\n",
    "names = ['basic', 'lemma', 'norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ee319ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2838, 11516) (2838, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_vector.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03999716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic 0.6461538461538462\n",
      "lemma 0.6643356643356644\n",
      "norm 0.6559440559440559\n"
     ]
    }
   ],
   "source": [
    "# Try to fit and evaluate a Multinomial Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model_NB = MultinomialNB()\n",
    "\n",
    "for i, data in enumerate(data_sets):\n",
    "    model_NB.fit(data[0], y_train.label)\n",
    "    score = model_NB.score(data[1],y_test.label)\n",
    "    print(names[i],score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfd95e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic, rbf, 0.722\n",
      "basic, poly, 0.692\n",
      "lemma, rbf, 0.733\n",
      "lemma, poly, 0.705\n",
      "norm, rbf, 0.733\n",
      "norm, poly, 0.627\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "kernels = ['rbf', 'poly']\n",
    "for i, data in enumerate(data_sets):\n",
    "    for kernel in kernels:\n",
    "        model_SVC = SVC(kernel= kernel)\n",
    "        model_SVC.fit(data[0], y_train.label)\n",
    "        score = model_SVC.score(data[1],y_test.label)\n",
    "        print(names[i], kernel, round(score, 3), sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f6728ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic, 0.708\n",
      "lemma, 0.697\n",
      "norm, 0.701\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model_gb = GradientBoostingClassifier()\n",
    "#     random_state=4, subsample=0.8, max_features=\"auto\", warm_start=True)\n",
    "\n",
    "for i, data in enumerate(data_sets):\n",
    "        model_gb.fit(data[0], y_train.label)\n",
    "        score = model_gb.score(data[1],y_test.label)\n",
    "        print(names[i], round(score, 3), sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0f323a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lem_text</th>\n",
       "      <th>norm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He said he had not felt that way before, sugge...</td>\n",
       "      <td>he say he have not feel that way before , sugg...</td>\n",
       "      <td>say not feel way suggete go rest trigger ahead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey there r/assistance, Not sure if this is th...</td>\n",
       "      <td>hey there r / assistance , not sure if this be...</td>\n",
       "      <td>hey r assistance not sure right place post go ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My mom then hit me with the newspaper and it s...</td>\n",
       "      <td>my mom then hit I with the newspaper and it sh...</td>\n",
       "      <td>mom hit newspaper shock would know not like pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>until i met my new boyfriend, he is amazing, h...</td>\n",
       "      <td>until I meet my new boyfriend , he be amazing ...</td>\n",
       "      <td>meet new boyfriend amazing kind sweet good stu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>October is Domestic Violence Awareness Month a...</td>\n",
       "      <td>October be Domestic Violence Awareness Month a...</td>\n",
       "      <td>october domestic violence awareness month dome...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  He said he had not felt that way before, sugge...   \n",
       "1  Hey there r/assistance, Not sure if this is th...   \n",
       "2  My mom then hit me with the newspaper and it s...   \n",
       "3  until i met my new boyfriend, he is amazing, h...   \n",
       "4  October is Domestic Violence Awareness Month a...   \n",
       "\n",
       "                                            lem_text  \\\n",
       "0  he say he have not feel that way before , sugg...   \n",
       "1  hey there r / assistance , not sure if this be...   \n",
       "2  my mom then hit I with the newspaper and it sh...   \n",
       "3  until I meet my new boyfriend , he be amazing ...   \n",
       "4  October be Domestic Violence Awareness Month a...   \n",
       "\n",
       "                                           norm_text  \n",
       "0  say not feel way suggete go rest trigger ahead...  \n",
       "1  hey r assistance not sure right place post go ...  \n",
       "2  mom hit newspaper shock would know not like pl...  \n",
       "3  meet new boyfriend amazing kind sweet good stu...  \n",
       "4  october domestic violence awareness month dome...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb50f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10c7c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
